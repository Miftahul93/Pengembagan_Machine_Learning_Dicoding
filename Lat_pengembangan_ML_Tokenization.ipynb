{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lat pengembangan ML-Tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP53RzOYYRG7bUMRrUnLyW3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Miftahul93/Pengembangan_Machine_Learning_Dicoding/blob/main/Lat_pengembangan_ML_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62eAHkfTw53W"
      },
      "source": [
        "# **Latihan Tokenization**\n",
        "Pada latihan kali ini kita akan belajar bagaimana melakukan tokenization dan membuat sequence dari teks kita. Untuk menggunakan tokenizer, impor library di bawah."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuIyam23w12U"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejZNwxN1xdbi"
      },
      "source": [
        "Kemudian buat objek tokenizer dengan memanggil fungsi tokenizer dan melengkapi parameternya. Parameter num_words adalah jumlah kata yang akan dikonversi ke dalam token/bilangan numerik. Jika parameter num_words diisi 15, maka hanya 15 huruf yang paling sering muncul akan ditokenisasi dari seluruh kata pada dataset. \n",
        "\n",
        "Sedangkan parameter oov_token adalah parameter yang berfungsi untuk mengganti kata-kata yang tidak ditokenisasi menjadi karakter tertentu. Pada praktiknya, lebih baik untuk mengganti kata yang tidak dikenali dengan suatu kata tertentu dibanding melewatkan kata tersebut untuk mengurangi informasi yang hilang. Hal inilah yang dapat dilakukan dengan menambahkan parameter OOV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dsPh_ZLxgpy"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=15, oov_token='-')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoDdNAUA1FlP"
      },
      "source": [
        "Lalu, buat teks yang akan kita tokenisasi dan kita pakai untuk pelatihan model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajZx_kY71FLr"
      },
      "source": [
        "teks = ['Saya suka programing',\n",
        "        'Programing sangat menyenangkan!',\n",
        "        'Machine learning berbeda dengan pemrograman konvensional']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cURMoeht1K5-"
      },
      "source": [
        "Untuk melakukan tokenisasi, panggil fungsi fit_on_text() pada objek tokenizer dan isi teks kita sebagai argumennya."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aAct4mj1KjT"
      },
      "source": [
        "tokenizer.fit_on_texts(teks)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfDEcyEM17G9"
      },
      "source": [
        "Kemudian, kita akan mengubah kalimat ke dalam nilai yang sesuai dengan fungsi texts_to_sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW-QORaT16sm"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(teks)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcUGxHwZ2hIg"
      },
      "source": [
        "Untuk melihat hasil tokenisasi, kita bisa memanggil atribut word_index dari objek tokenizer. atribut word index mengembalikan dictionary berupa kata sebagai key dan token atau nilai numeriknya sebagai value. Perlu diperhatikan bahwa tanda baca dan huruf kapital tidak diproses oleh tokenizer. Contohnya kata “Selamat!” dan “SELAMAT” akan diperlakukan sebagai kata yang sama. Hasil dari cell di bawah menunjukkan bahwa kata-kata yang out-of-vocabulary akan diberi token bernilai 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7zq4xob2gwB",
        "outputId": "07c07f08-117e-4264-dc2c-642201b39e87"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'-': 1, 'programing': 2, 'saya': 3, 'suka': 4, 'sangat': 5, 'menyenangkan': 6, 'machine': 7, 'learning': 8, 'berbeda': 9, 'dengan': 10, 'pemrograman': 11, 'konvensional': 12}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-ImFYI_4pkT"
      },
      "source": [
        "Contoh OOV seperti di bawah. Kata ‘belajar’, ‘sejak’, dan ‘SMP’ tidak ada memiliki token pada dictionary hasil tokenisasi. Tanpa OOV, sequence yang dihasilkan akan seperti output pada baris pertama di mana, kata yang tidak memiliki token tidak dimasukkan pada sequence. Jika kita menggunakan OOV, maka setiap kata yang tidak memiliki token akan diberikan token yang seragam. Dengan OOV, informasi urutan setiap kata pada kalimat tidak hilang."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gQp43r94pN8",
        "outputId": "30dab54c-a313-4687-88db-821486ebc6db"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(['Saya suka programming']))\n",
        "print(tokenizer.texts_to_sequences(['Saya suka belajar programing seja SMP']))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 4, 1]]\n",
            "[[3, 4, 1, 2, 1, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLDZLxrT6F6c"
      },
      "source": [
        "Setelah tokenisasi, untuk mengubah kalimat ke dalam nilai-nilai yang sesuai dapat dengan menggunakan fungsi text_to_sequence() dan masukkan parameter dari teks kita. Ketika sequence telah dibuat, hal yang perlu kita lakukan adalah padding. Yup, padding adalah proses untuk membuat setiap kalimat pada teks memiliki panjang yang seragam. Sama seperti melakukan resize gambar, agar resolusi setiap gambar sama besar. Untuk menggunakan padding impor library pad_sequence. Kemudian buat panggil fungsi pad_sequence() dan masukkan sequence hasil tokenisasi sebagai parameternya."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQgse8WK6Feh"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "sequences_samapanjang = pad_sequences(sequences)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5LxDmHG6yg9"
      },
      "source": [
        "Hasil setelah padding adalah setiap sequence memiliki panjang yang sama. Padding dapat melakukan ini dengan menambahkan 0 secara default pada awal sequence yang lebih pendek"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VRXtR6i6yFo",
        "outputId": "f4edf66b-c8ab-415e-9d4c-767b9960faf9"
      },
      "source": [
        "print(sequences_samapanjang)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  3  4  2]\n",
            " [ 0  0  0  2  5  6]\n",
            " [ 7  8  9 10 11 12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITsrvGEe7nsL"
      },
      "source": [
        "Jika kita ingin merubah sehingga 0 ditambahkan di akhir sequence, kita dapat menggunakan parameter padding dengan nilai ‘post’. Selain itu kita dapat mengatur berapa maksimum panjang setiap sequence dengan parameter maxlen dan nilai yang kita inginkan. Jika kita mengisi nilai 5, maka panjang sebuah sequence tidak akan melebihi 5. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUHIg5eZ7nXL"
      },
      "source": [
        "sequences_samapanjang = pad_sequences(sequences,\n",
        "                                      padding='post',\n",
        "                                      maxlen=5)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY2f-X_W8Bdm"
      },
      "source": [
        "Jika teks kita memiliki panjang lebih dari nilai parameter maxlen misalnya 5, maka secara default nilai dari sequence akan diambil 5 nilai terakhir atau 5 kata terakhir saja dari setiap kalimat. Untuk mengubah pengaturan ini dan mengambil 5 kata terakhir dari tiap kalimat, kita dapat menggunakan parameter truncating dan mengisi nilai ‘post’."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYVoyGf48BG9"
      },
      "source": [
        "sequences_samapanjang = pad_sequences(sequences,\n",
        "                                      padding='post',\n",
        "                                      maxlen=5,\n",
        "                                      truncating='post')"
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}